# Word Embedding Visualization

**The code in this project visualizes how neural networks learn word embeddings.**

In the first example, you can see how ten words are randomly initialized in a 2-dimensional space and how the model then learns to optimize each word's embedding, so that words with the same POS tags are grouped closer together. 
<img src="videos/gif_no_background.gif" width="600"/>

In this visualization, you can additionally see how the model learns to classify words into categories according to their POS tags.
<img src="videos/gif_1.gif" width="600"/>

The last video example shows that--depending on different random initializations--different representations and classification boundaries are being learned.
<img src="videos/gif_2.gif" width="600"/>


